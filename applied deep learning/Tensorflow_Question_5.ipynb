{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KyPQnkZ7vT23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab65e3ba-4042-407b-b989-7d34f5766a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.3.0-py3-none-any.whl (167 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.11.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (7.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from keras-tuner) (2.25.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.2)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.4.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.19.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (4.5.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (2.11.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (3.1.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (15.0.6.1)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (23.1.21)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.22.4)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.0->keras-tuner) (1.51.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (5.7.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2022.12.7)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->keras-tuner) (1.26.14)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->keras-tuner) (0.38.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython->keras-tuner) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->keras-tuner) (0.2.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.16.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.2.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.4.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.0->keras-tuner) (3.2.2)\n",
            "Installing collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.18.2 keras-tuner-1.3.0 kt-legacy-1.0.4\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "!pip install keras-tuner\n",
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Datasets/Insurance_claims_Final.csv')\n"
      ],
      "metadata": {
        "id": "cqmagBK_vbzJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ItdQut7BvcdK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef62b6f-96d8-446f-92a9-1fcaa6b6a453"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning data for model"
      ],
      "metadata": {
        "id": "453yK_Ep0tGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean for nan values in LossHour and PolicyHolderAge columns\n",
        "df[['LossHour', 'PolicyHolderAge' , 'FpVehicleAgeMonths']] = df[['LossHour', 'PolicyHolderAge' , 'FpVehicleAgeMonths']].fillna(df[['LossHour', 'PolicyHolderAge' , 'FpVehicleAgeMonths']].mean())\n",
        "\n",
        "# Dropping columns\n",
        "df = df.drop(['ThirdPartyVehicleNumber', 'InsurerNotes', 'DamageImportance' , 'ConnectionBetweenParties'], axis=1)\n",
        "\n",
        "# Filling low nan values column with mode\n",
        "df['FirstPartyVehicleType'].fillna(df['FirstPartyVehicleType'].mode()[0], inplace=True)\n",
        "\n",
        "df['LossDate'] = pd.to_datetime(df['LossDate'], infer_datetime_format=True)\n",
        "df['LossDate'] = df['LossDate'].dt.strftime('%d-%m-%Y')\n",
        "df['FirstPolicySubscriptionDate'] = pd.to_datetime(df['FirstPolicySubscriptionDate'], infer_datetime_format=True)\n",
        "df['FirstPolicySubscriptionDate'] = df['FirstPolicySubscriptionDate'].dt.strftime('%d-%m-%Y')\n",
        "\n",
        "constant = 1\n",
        "df['claim_amount_log'] = np.log(df['ClaimAmount'] + constant)\n",
        "\n"
      ],
      "metadata": {
        "id": "RIBGCK3XwOTW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['ClaimAmount'] , axis=1)\n",
        "cat_df = df.select_dtypes(include = ['object']).copy()\n",
        "\n",
        "df.PolicyholderOccupation = pd.Categorical(df.PolicyholderOccupation).codes\n",
        "df.ClaimCause = pd.Categorical(df.ClaimCause).codes\n",
        "df.ClaimInvolvedCovers = pd.Categorical(df.ClaimInvolvedCovers).codes\n",
        "df.FirstPartyVehicleType = pd.Categorical(df.FirstPartyVehicleType).codes\n",
        "df.PolicyHolderPostCode = pd.Categorical(df.PolicyHolderPostCode).codes\n",
        "\n",
        "#There were string values in the code which is changed to nan values\n",
        "for i in range(len(df)):\n",
        "    if isinstance(df.loc[i, 'LossPostCode'], str) and not df.loc[i, 'LossPostCode'].isnumeric():\n",
        "        df.loc[i, 'LossPostCode'] = np.nan\n",
        "    else:\n",
        "        df.loc[i, 'LossPostCode'] = float(df.loc[i, 'LossPostCode'])\n",
        "\n",
        "#Nan values are changed to 0\n",
        "df[['LossPostCode']] = df[['LossPostCode']].fillna(0)\n",
        "\n",
        "df = df.drop(['LossDate' , 'FirstPolicySubscriptionDate'] , axis=1)\n",
        "\n",
        "df[['FirstPartyVehicleNumber', 'LossPostCode']] = df[['FirstPartyVehicleNumber', 'LossPostCode']].fillna(0)\n",
        "\n"
      ],
      "metadata": {
        "id": "ThAiUR-pwilF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X1 = df.drop('Fraud', axis=1)  # drop the target variable\n",
        "y1 = df['Fraud']"
      ],
      "metadata": {
        "id": "kOBHHSmTwOh6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data and creating model"
      ],
      "metadata": {
        "id": "l1Xu4-Ec1A7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.15, random_state=172)\n",
        "X1_train, X1_valid, y1_train, y1_valid = train_test_split(X1_train, y1_train, test_size=0.8, random_state=172)\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation=\"relu\", kernel_initializer='uniform'),\n",
        "    tf.keras.layers.Dense(32, activation=\"relu\", kernel_initializer='uniform'),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\", kernel_initializer='uniform')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "log_1 = model.fit(X1_train, y1_train, epochs=100, batch_size=256, validation_data=(X1_valid, y1_valid))\n",
        "\n",
        "def train_model(hp):    \n",
        "    num_units = hp.Int('num_units', min_value = 32, max_value=256) \n",
        "    dropout_rate = hp.Float('dropout_rate', min_value = 0.1, max_value=0.3) \n",
        "    learning_rate = hp.Float('learning_rate', min_value = 0.001, max_value=0.1, sampling='log') \n",
        "    \n",
        "    model = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(num_units, activation=\"relu\"),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(num_units, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[tf.keras.metrics.BinaryAccuracy(), tf.keras.metrics.AUC()])\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEI2TNc8ph24",
        "outputId": "52d3e1ae-3e12-41ce-c130-0b6dab6d7664"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "8/8 [==============================] - 5s 124ms/step - loss: 8.6269 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 3.7164 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 2/100\n",
            "8/8 [==============================] - 0s 55ms/step - loss: 6.6701 - precision: 0.0093 - recall: 0.0909 - val_loss: 3.7961 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 3/100\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 6.2085 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 6.6014 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 4/100\n",
            "8/8 [==============================] - 0s 29ms/step - loss: 8.6206 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 6.9235 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 5/100\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 8.4638 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 6.1966 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 6/100\n",
            "8/8 [==============================] - 0s 56ms/step - loss: 7.1485 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 4.5123 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 7/100\n",
            "8/8 [==============================] - 0s 66ms/step - loss: 4.7001 - precision: 0.2000 - recall: 0.0455 - val_loss: 2.2193 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 8/100\n",
            "8/8 [==============================] - 0s 50ms/step - loss: 1.9846 - precision: 0.0213 - recall: 0.1818 - val_loss: 1.4502 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 9/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.6207 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.7916 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 10/100\n",
            "8/8 [==============================] - 0s 40ms/step - loss: 3.6298 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.6978 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 11/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 3.1543 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 2.0186 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 12/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 2.1692 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.1066 - val_precision: 0.0296 - val_recall: 0.0704\n",
            "Epoch 13/100\n",
            "8/8 [==============================] - 0s 69ms/step - loss: 0.9230 - precision: 0.1667 - recall: 0.0455 - val_loss: 0.2483 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 14/100\n",
            "8/8 [==============================] - 1s 77ms/step - loss: 0.4456 - precision: 0.0253 - recall: 0.0909 - val_loss: 0.5869 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 15/100\n",
            "8/8 [==============================] - 0s 57ms/step - loss: 0.9297 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.7420 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 16/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.8170 - precision: 0.5000 - recall: 0.0455 - val_loss: 0.4331 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 17/100\n",
            "8/8 [==============================] - 1s 73ms/step - loss: 0.3657 - precision: 0.0357 - recall: 0.0455 - val_loss: 0.2675 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 18/100\n",
            "8/8 [==============================] - 0s 47ms/step - loss: 0.2394 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.2523 - val_precision: 0.0268 - val_recall: 0.1127\n",
            "Epoch 19/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.3572 - precision: 0.0833 - recall: 0.0455 - val_loss: 0.3172 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 20/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.1767 - precision: 0.5000 - recall: 0.0455 - val_loss: 0.2461 - val_precision: 0.0234 - val_recall: 0.0986\n",
            "Epoch 21/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1177 - precision: 0.2000 - recall: 0.0455 - val_loss: 0.1251 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 22/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.2151 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1299 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 23/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.3124 - precision: 0.0588 - recall: 0.0455 - val_loss: 0.5379 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 24/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.7419 - precision: 0.3333 - recall: 0.0455 - val_loss: 0.5063 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 25/100\n",
            "8/8 [==============================] - 1s 78ms/step - loss: 0.4351 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0990 - val_precision: 0.0192 - val_recall: 0.0282\n",
            "Epoch 26/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.9900 - precision: 0.0310 - recall: 0.1818 - val_loss: 1.5249 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 27/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 2.2643 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 1.8002 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 28/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 2.2713 - precision: 0.5000 - recall: 0.0455 - val_loss: 1.5912 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 29/100\n",
            "8/8 [==============================] - 0s 36ms/step - loss: 1.8719 - precision: 0.2500 - recall: 0.0455 - val_loss: 1.2325 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 30/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 1.4276 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_loss: 0.8851 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 31/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.9540 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.5678 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 32/100\n",
            "8/8 [==============================] - 0s 61ms/step - loss: 0.6213 - precision: 0.5000 - recall: 0.0455 - val_loss: 0.2843 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 33/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.2006 - precision: 0.1250 - recall: 0.0455 - val_loss: 0.1973 - val_precision: 0.0090 - val_recall: 0.0423\n",
            "Epoch 34/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1648 - precision: 0.0588 - recall: 0.0455 - val_loss: 0.1473 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 35/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.1198 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0904 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 36/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0887 - precision: 0.2000 - recall: 0.0455 - val_loss: 0.0897 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 37/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.1006 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1129 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 38/100\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.1082 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1121 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 39/100\n",
            "8/8 [==============================] - 0s 18ms/step - loss: 0.1036 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0800 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 40/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0782 - precision: 0.1429 - recall: 0.0455 - val_loss: 0.1078 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 41/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1102 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1301 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 42/100\n",
            "8/8 [==============================] - 0s 64ms/step - loss: 0.1191 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1259 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 43/100\n",
            "8/8 [==============================] - 1s 75ms/step - loss: 0.1013 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0847 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 44/100\n",
            "8/8 [==============================] - 0s 63ms/step - loss: 0.0736 - precision: 0.3333 - recall: 0.0455 - val_loss: 0.0744 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 45/100\n",
            "8/8 [==============================] - 0s 39ms/step - loss: 0.0650 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0734 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 46/100\n",
            "8/8 [==============================] - 0s 42ms/step - loss: 0.0702 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0699 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 47/100\n",
            "8/8 [==============================] - 0s 41ms/step - loss: 0.1239 - precision: 0.1111 - recall: 0.0455 - val_loss: 0.1634 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 48/100\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 0.1316 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1899 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 49/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1565 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1918 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 50/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.1432 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1662 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 51/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.1153 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1290 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 52/100\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.1075 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1116 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 53/100\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.1023 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0931 - val_precision: 0.0081 - val_recall: 0.0141\n",
            "Epoch 54/100\n",
            "8/8 [==============================] - 0s 26ms/step - loss: 0.0814 - precision: 0.5000 - recall: 0.0455 - val_loss: 0.0781 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 55/100\n",
            "8/8 [==============================] - 0s 48ms/step - loss: 0.0775 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0702 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 56/100\n",
            "8/8 [==============================] - 0s 35ms/step - loss: 0.0682 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0677 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 57/100\n",
            "8/8 [==============================] - 1s 68ms/step - loss: 0.0646 - precision: 0.5000 - recall: 0.0455 - val_loss: 0.0811 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 58/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0806 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0822 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 59/100\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 0.0681 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0747 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 60/100\n",
            "8/8 [==============================] - 0s 44ms/step - loss: 0.0812 - precision: 0.1429 - recall: 0.0455 - val_loss: 0.0976 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 61/100\n",
            "8/8 [==============================] - 0s 19ms/step - loss: 0.0863 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1031 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 62/100\n",
            "8/8 [==============================] - 0s 33ms/step - loss: 0.0879 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0959 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 63/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.0973 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0746 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 64/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0780 - precision: 0.1429 - recall: 0.0455 - val_loss: 0.1051 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 65/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0915 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1176 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 66/100\n",
            "8/8 [==============================] - 0s 28ms/step - loss: 0.1576 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1053 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 67/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0826 - precision: 0.2500 - recall: 0.0455 - val_loss: 0.0890 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 68/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.0843 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1085 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 69/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0771 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0663 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 70/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0654 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0763 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 71/100\n",
            "8/8 [==============================] - 0s 34ms/step - loss: 0.0704 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0638 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 72/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0682 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0723 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 73/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0804 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0768 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 74/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0624 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0664 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 75/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0636 - precision: 0.2500 - recall: 0.0455 - val_loss: 0.0998 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 76/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0868 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0858 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 77/100\n",
            "8/8 [==============================] - 0s 20ms/step - loss: 0.0765 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0853 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 78/100\n",
            "8/8 [==============================] - 0s 21ms/step - loss: 0.0739 - precision: 0.1667 - recall: 0.0455 - val_loss: 0.0840 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 79/100\n",
            "8/8 [==============================] - 0s 25ms/step - loss: 0.0865 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0935 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 80/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0815 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.1081 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 81/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0770 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0719 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 82/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0751 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0939 - val_precision: 0.0652 - val_recall: 0.0423\n",
            "Epoch 83/100\n",
            "8/8 [==============================] - 0s 32ms/step - loss: 0.0804 - precision: 0.2000 - recall: 0.0455 - val_loss: 0.0761 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 84/100\n",
            "8/8 [==============================] - 0s 23ms/step - loss: 0.0812 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0719 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 85/100\n",
            "8/8 [==============================] - 0s 22ms/step - loss: 0.0656 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0635 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 86/100\n",
            "8/8 [==============================] - 0s 30ms/step - loss: 0.0582 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0719 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 87/100\n",
            "8/8 [==============================] - 0s 31ms/step - loss: 0.0725 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0676 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 88/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0617 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0636 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 89/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0599 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0650 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 90/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0599 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0745 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 91/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0716 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0700 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 92/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0695 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0697 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 93/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0882 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0800 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 94/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0635 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0646 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 95/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0602 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0648 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 96/100\n",
            "8/8 [==============================] - 0s 15ms/step - loss: 0.0818 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0766 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 97/100\n",
            "8/8 [==============================] - 0s 14ms/step - loss: 0.0659 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0678 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 98/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0616 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0627 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 99/100\n",
            "8/8 [==============================] - 0s 16ms/step - loss: 0.0597 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0686 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
            "Epoch 100/100\n",
            "8/8 [==============================] - 0s 17ms/step - loss: 0.0626 - precision: 1.0000 - recall: 0.0455 - val_loss: 0.0640 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(log_1.history['loss'],label = \"training loss\")\n",
        "plt.plot(log_1.history['val_loss'], label = \"validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "UqBkd5-OqcXu",
        "outputId": "d6388d50-dbf8-4a26-f5b0-9a4c94a774ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "c51Es8pZ1LfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X1_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqUuobcDKQ_M",
        "outputId": "43310249-1aed-4d9e-84d7-447be119202d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = tf.math.confusion_matrix(y1_test, y_pred)\n",
        "tn, fp, fn, tp = conf_matrix.numpy().ravel()\n",
        "hit_rate = tp / (tp + fn)\n",
        "decision_rate = (tp + fp) / (tp + tn + fp + fn)\n",
        "print(\"Hit rate:\", hit_rate)\n",
        "print(\"Decision rate:\", decision_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcpGAdLTKFI2",
        "outputId": "7d1dc9a1-cbe6-48d7-f8f2-99332dba6479"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit rate: 0.0\n",
            "Decision rate: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper parameter tuning to find best best parameters"
      ],
      "metadata": {
        "id": "8Y4FYbo71TFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuner = kt.Hyperband(train_model,\n",
        "                     objective='val_loss',\n",
        "                     max_epochs=5,\n",
        "                     factor=3,\n",
        "                     directory='logs',\n",
        "                     project_name='kt_tutorial_2')\n",
        "\n",
        "tuner.search(X1_train, y1_train, validation_data=(X1_valid, y1_valid))\n",
        "\n",
        "best_hps = tuner.get_best_hyperparameters()[0]\n",
        "print(\"Best number of hidden units:\", best_hps['num_units'])\n",
        "print(\"Best dropout rate:\", best_hps['dropout_rate'])\n",
        "print(\"Best learning rate:\", best_hps['learning_rate'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtI7kNwwI_PJ",
        "outputId": "dfc97a1c-7a50-4591-9116-31f48d77b2f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 07s]\n",
            "val_loss: 43.12047576904297\n",
            "\n",
            "Best val_loss So Far: 0.05161834880709648\n",
            "Total elapsed time: 00h 01m 05s\n",
            "Best number of hidden units: 69\n",
            "Best dropout rate: 0.2916418759634546\n",
            "Best learning rate: 0.07075066125528888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "dxS076Wd1dN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "best_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkLA0SV-34iK",
        "outputId": "0951411f-5966-4895-f864-d1e06fb62934"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.sequential.Sequential at 0x7f12c1d47c10>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_1 = best_model.predict(X1_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UjAwJkz2F3J",
        "outputId": "238b6905-da3c-42f0-9952-74751c0117eb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 0s 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "CP0UFJ8i4ZwB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training best model"
      ],
      "metadata": {
        "id": "4Rpr8Nq91uCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M1=best_model.fit(X1_train, y1_train,\n",
        "               epochs=30,\n",
        "               validation_data=(X1_valid,y1_valid),\n",
        "               callbacks=[early_stopping_cb])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ow4G6r24PNU",
        "outputId": "072e0942-e25c-48b1-e4f8-c6d2865c4c14"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "62/62 [==============================] - 7s 73ms/step - loss: 13453.6279 - binary_accuracy: 0.9597 - auc_2: 0.4860 - val_loss: 1.1144 - val_binary_accuracy: 0.9884 - val_auc_2: 0.5024\n",
            "Epoch 2/30\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 184.1464 - binary_accuracy: 0.9816 - auc_2: 0.5082 - val_loss: 0.0530 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 3/30\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 37.8725 - binary_accuracy: 0.9867 - auc_2: 0.4130 - val_loss: 0.0520 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 4/30\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 12.0788 - binary_accuracy: 0.9867 - auc_2: 0.4969 - val_loss: 0.0517 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 5/30\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.0621 - binary_accuracy: 0.9888 - auc_2: 0.5535 - val_loss: 0.0525 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 6/30\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3488 - binary_accuracy: 0.9878 - auc_2: 0.4535 - val_loss: 0.0516 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 7/30\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.0620 - binary_accuracy: 0.9888 - auc_2: 0.4355 - val_loss: 0.0517 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 8/30\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 0.0620 - binary_accuracy: 0.9888 - auc_2: 0.4786 - val_loss: 0.0516 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 9/30\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.0619 - binary_accuracy: 0.9888 - auc_2: 0.5345 - val_loss: 0.0521 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 10/30\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.0595 - binary_accuracy: 0.9893 - auc_2: 0.4755 - val_loss: 0.0517 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 11/30\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.0621 - binary_accuracy: 0.9888 - auc_2: 0.4643 - val_loss: 0.0518 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 12/30\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 6.7941 - binary_accuracy: 0.9888 - auc_2: 0.4585 - val_loss: 0.0517 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n",
            "Epoch 13/30\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.0619 - binary_accuracy: 0.9888 - auc_2: 0.4725 - val_loss: 0.0516 - val_binary_accuracy: 0.9909 - val_auc_2: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.evaluate(X1_test, y1_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n9MReqF4QbS",
        "outputId": "a7ccef7d-cbd6-4978-dc7f-255cde32b377"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 0s 3ms/step - loss: 0.0691 - binary_accuracy: 0.9873 - auc_2: 0.5000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0690934807062149, 0.9872832298278809, 0.5]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(M1.history['loss'], label='training loss')\n",
        "plt.plot(M1.history['val_loss'], label='validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "G9DD-N7GNzPm",
        "outputId": "beade70a-b993-4bf7-b3dc-81726741531b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = tf.math.confusion_matrix(y1_test, y_pred_1)\n",
        "tn, fp, fn, tp = conf_matrix.numpy().ravel()\n",
        "hit_rate = tp / (tp + fn)\n",
        "decision_rate = (tp + fp) / (tp + tn + fp + fn)\n",
        "print(\"Hit rate:\", hit_rate)\n",
        "print(\"Decision rate:\", decision_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBQxxnDmTkKy",
        "outputId": "e94f0e3b-941f-4217-bb02-6e89b60130fb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit rate: 0.0\n",
            "Decision rate: 0.0\n"
          ]
        }
      ]
    }
  ]
}